# 第三章 线性神经网络

## 一些概念

**特征向量**：我们可以通过样本的基本属性或特征代表样本本身，比如通过苹果的色泽、形状大小、气味、味道等特征来反映一个苹果，为了方便计算机使用这些特征而把这些特征以数字的形式表征，这组数字构成的向量就称为特征向量。

**标签**：对样本做标记，这个标记就称为标签。比如把形状大的、颜色红的苹果标记为“好苹果”，则这个“好苹果”就是这个样本的标签。

**训练样本**：由特征和标签构成，记作$(x,y)$，其中$x$为样本特征，$y$为样本标签。

**损失函数**：量化模型预测值与目标实际值之间的差距，通常选择非负数，数值越小说明预测值越接近真实值。

**独热编码**：在分类问题中用于表示标签的一种方法，构造一组向量，其中数量和分量都和标签类别一样多，类别所对应的分量为1，其余为0。由于这组向量是相互正交的，因此独热编码默认标签之间是没有关联的。

**超参数**：需要手动设置的参数，不随着模型训练改变。

**仿射变换**：一种带有偏置项的线性变换，可以通过一个线性变换矩阵$A$和一个平移向量$b$来表示，$f:x\to Ax + b$。

## 梯度下降法：

假设我们关心的函数是总损失函数，一般的总损失函数是由所有训练样本的损失均值构成，为了找到最优的参数使得总损失函数达到最小值，我们采用梯度下降算法。

原理是计算总损失函数关于模型参数的梯度，沿着梯度的反方向更新参数时损失函数下降最快。

数值上通过迭代的方式，不断计算梯度，更新参数的方式使得损失函数不断下降，接近最优点。

梯度下降法的更新公式如下：
$$
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla_{\theta} J(\theta^{(t)}) \\
J(\theta^{(t)}) = \frac{1}{\sum_i} \sum_i J(\theta^{(t)}, x_i, y_i)
$$
这里为$J(\theta)$损失函数，$J(\theta, x_i, y_i)$是训练样本$(x_i,y_i)$的损失，$\theta$为参数，$\alpha$为学习率，而$\theta^{(t)}$是在第t次迭代中的参数值。



## 随机梯度下降法：

在梯度下降算法中，每次更新参数需要遍历一次全部训练样本，这个过程十分耗时。

随机梯度下降算法，则是每次随机抽取部分训练样本而不是全部，以这部分训练样品的损失平均作为一个临时损失函数，根据这个临时损失函数，进行一次梯度下降的参数更新。

因为只取部分样本进行梯度下降，受到样本噪声的影响较大，所以参数更新的方向与用全部样本的更新方向会稍微不同，但是随着不断随机抽取，抽取的部分训练样本是可以反映所有训练样本的，因此，整体而言，随机梯度下降算法还是会使得参数往最优的方向，即所有训练样本的损失函数最小的方向移动。



梯度下降法的更新公式如下：
$$
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla_{\theta} J^*(\theta^{(t)}) \\
J^*(\theta^{(t)}) = \frac{1}{\sum_i} \sum_i J(\theta^{(t)}, x_i, y_i)
$$
这里求和是取的随机抽取中的样本。特别地，当求和取所有样本时回到梯度下降。有时候为了做区分，把随机抽取的样本数大于1时的算法称为小批量随机梯度，把随机抽取的样本数等于1时的称为随机梯度。

优点：处理大规模数据时更加高效，并且由于受样本噪声的影响，在某些情况下能帮助避免局部最小值。

缺点：受样本噪声的影响，不会完全落入最优点，而在其附近徘徊。

在实际使用中，一般是随机将样本分成几部分(尽可能相等)，每次取其中一部分的样本更新参数，取遍所有部分后完成一个回合的参数更新，接着重复操作，再次将样本重新做分配。



## 均方损失与极大似然估计

**对于回归问题，一般采用均方误差作为损失函数。**下面从极大似然估计角度出发进行分析。

考虑简单的线性回归模型，
$$
y = \mathbf{w}^{\top} \mathbf{x} + b + \epsilon,
$$
其中，噪声满足正态分布，$\epsilon\sim \mathcal{N}(0,\sigma^2)$。

输入样本特征$\mathbf{x}$，此时模型的输出为$\mathbf{w}^{\top} \mathbf{x} + b + \epsilon$，它等于特定$y$的概率为，
$$
p(y|\mathbf{x}) = p(\epsilon=y-\mathbf{w}^{\top} \mathbf{x}-b) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{1}{2\sigma^2}(y-\mathbf{w}^{\top} \mathbf{x}-b)^2 \right)}.
$$
现在有一组训练数据集$\{(\mathbf{x}_i,y_i)\}$，根据极大似然估计法估计参数$\mathbf{w},b$，那么就是要找到最优的$\mathbf{w},b$，使得整个数据集的似然函数达到最大，
$$
P(\mathbf{Y}|\mathbf{X})=\prod_{i}p(y_i|\mathbf{x}_i),
$$
等价于求$-\log P(\mathbf{Y}|\mathbf{X})$最小值，
$$
-\log P(\mathbf{Y}|\mathbf{X}) = \sum_{i=1}^n\frac{1}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2}(y_i - \mathbf{w}^\top_i - b)^2.
$$
可以看到，第二项正是均方误差。**在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计，它们给出一样的参数$\mathbf{w},b$。**



## softmax函数

softmax函数能够将未规范化的预测变换为非负数，且总和为1，同时让模型保持可导的性质，满足概率分布的要求。简单地说softmax函数就是把模型输出映射成一个概率分布，根据分布输出预测的类别。

softmax函数定义：
$$
y_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)},
$$
其中，$\{x_i\}$是未规范的输出，$\{y_i\}$是经过softmax函数处理后的输出。



## 交叉熵损失与极大似然估计

**对于分类问题，一般采用交叉熵损失作为损失函数。**下面从极大似然估计角度出发进行分析。

假设有$n$个训练样本$\{\mathbf{(x^{(i)},y^{(i)})}\}_{i=1}^n$，采用独热编码，若标签$\mathbf{y^{(i)}}$属于第$k_i$个类别，第$k_i$个分量上为1，其余分量为0。

记模型的输入为$\mathbf{x^{(i)}}$时，输出的预测为$\mathbf{\hat{y}^{(i)}}$，它表示每个类别的概率。那么，模型预测与标签属于同一个类别的概率为：
$$
p_{w}(\mathbf{y^{(i)}}|\mathbf{x^{(i)}}) = \mathbf{y^{(i)}}\cdot \mathbf{\hat{y}^{(i)}} = \sum_j y^{(i)}_j \hat{y}^{(i)}_j=\hat{y}^{(i)}_{k_i}
$$
其中$w$为模型的参数。

现在我们通过极大似然估计，估计模型中的参数。训练样本给出的似然函数为：
$$
P_w(\mathbf{Y}|\mathbf{X}) = \prod_i^n p_{w}(\mathbf{y^{(i)}}|\mathbf{x^{(i)}})
$$
最大化似然函数等价于最小化负对数似然：
$$
\begin{aligned}
-\log P_w(\mathbf{Y}|\mathbf{X}) &= -\sum_i^n \log p_{w}(\mathbf{y^{(i)}}|\mathbf{x^{(i)}}) \\
&= -\sum_i^n \log \hat{y}^{(i)}_{k_i} \\
&= -\sum_i^n \mathbf{y}^{(i)}\cdot \log \mathbf{\hat{y}^{(i)}}
\end{aligned}
$$
其中，最后一个等式是因为独热向量只有一个分量为1，其余为0。

交叉熵损失函数被定义为：
$$
l(\mathbf{y},\mathbf{\hat{y}})=\mathbf{y} \cdot \log \mathbf{\hat{y}} = -\sum_k y_k \log \hat{y}_k
$$
因此，从极大似然估计的角度看，对于分类问题，应该使用交叉熵作为损失函数。



**拓展：**

虽然以上是使用独热编码，但是对于非独热编码也是可以的。若使用非独热编码，则条件概率$p_{w}(\mathbf{y}|\mathbf{x})$需要修正，因为这时候$\mathbf{y}$不是一个类别，而是一个分布，无法用概率。

现在换一个角度，假如我们用模型做预测试验，当试验次数足够多时，出现类别$k$的频率接近或一样，则我们就可以说模型给出的预测分布是接近真实分布的。

因此，这里我们把条件概率$p_{w}(\mathbf{y}|\mathbf{x})$修改成$\prod_k p_{w}(y_k|\mathbf{x})$，。。。

==(这时候怎么办，我也没想明白)==

对于样本$(\mathbf{x},\mathbf{y})$，标签$\mathbf{y}$是一个向量，每个分量$y_k$代表属于该类的真实概率。假设使用了$N$次样本$\mathbf{x}$，且$N$足够大，则观测到是$k$类别的次数为$N\cdot y_k$。

模型的预测为$\mathbf{y}$，也是一个概率分布(列)。在预测的概率分布中，进行$N$次试验，发生有$N\cdot y_k$次观测到$k$类别的事件，$k=1,\cdots,l\text{（假设共有$l$个类别）}$，那么这个事件的概率为：
$$
p_{w}(\mathbf{y}|\mathbf{x}) = \prod_k \hat{y}_k^{N\cdot y_k}
$$
它表示模型的预测分布$\hat{\mathbf{y}}$正好是真实分布$\mathbf{y}$的概率，这正是条件概率$p_{w}(\mathbf{y}|\mathbf{x})$。
$$
\log p_w = \sum_k N\cdot y_k \log\hat{y}_k
$$
==P72P73==





## 从信息论理解交叉熵

**信息熵**：

一个分布列$\{P_i\}$的信息熵定义为：
$$
H(P)=\sum_i -P_i\log P_i
$$
其中，$-\log P_i$表示事件$i$发生时所得到的信息量，概率越小，事件的信息量越大。

因此，信息熵衡量的是从分布$P$中进行抽样或观测，得到的平均信息量，也就是所有事件信息量的期望。也可以理解为对分布$P$的无知程度，越无知，则从观测结果中得到的信息量越多。



**KL散度**：

为了衡量预测分布与真实分布之间的偏差或“距离”，定义K-L(Kullback-Leibler)散度。

设$\{P_i\}$是真实概率分布列，$\{Q_j\}$是观测或预测的分布列，则K-L散度被定义为：
$$
D_{KL}(P\|Q)=\sum_i P_i\log \frac{P_i}{Q_i} = \sum_i P_i\log {P_i} - \sum_i P_i\log {Q_i}
$$
KL散度的含义是，如果我们用预测分布$Q$来近似真实分布$P$时，所丢失的信息是多少。这里KL散度是以分布$P$作为参考进行衡量，因此也叫做相对熵。

补充理解：

- 假如$Q_i>P_i$，那么我们用预测分布$Q$代替真实分布时，对事件$i$发生的会过于自信，决策会比较激进。

- 假如$Q_i<P_i$，那么我们用预测分布$Q$代替真实分布时，对事件$i$发生的会过于不自信，决策会比较保守。

决策的激进和保守都是信息损失造成的，但决策激进承担的风险代价更大，因此，从$Q$到$P$的损失 与 从$P$到$Q$的损失 是不一样的，即KL散度是不对称。

性质：

- KL散度是不对称的，即$D_{KL}(P\|Q)=D_{KL}(Q\|P)$。

- $D_{KL}$是非负的，越小表示两个分布越接近。

  因为真实概率是认为不变的，



**交叉熵与KL散度的关系**：

从公式可以看出，交叉熵实际上可以分解为真实分布的熵和KL散度的和： 
$$
H(P,Q) = H(P) + D_{KL}(P\|Q) = - \sum_i P_i\log {Q_i}
$$
因此，想要预测分布接近真实分布就相当于最小化KL散度，而最小化KL散度$D_{KL}$等价于最小化交叉熵$H(P,Q)$。

**从信息论的角度，分类问题的损失函数应该用交叉熵损失。**



## softmax数值实现的一些问题

$\mathbf{x}$是未规范的预测，$\mathbf{y}$是其经过softmax函数后得到的概率分布，两者之间的关系为：
$$
y_j = \frac{\exp(x_j)}{\sum_k\exp(x_k)}
$$

1. **数值上溢**：由于指数函数的增长速度很快，$\exp(x_j)$很容易超过数据类型容许的最大值，造成计算机无法识别，显示 `Nan` ，这种情况称为上溢。

   **解决方法**：分子分母同时减小，避免数值过大，同时保证最终结果值不变，
   $$
   y_j = \frac{\exp(x_j- \max(x))  }{\sum_k\exp(x_k- \max(x))}
   $$

2. **数值下溢**：$\exp(x_j- \max(x))$有可能为很小的值，当小于数据类型容许的的范围时，计算机会进行四舍五入处理，使其为$0$，这种情况称为下溢。计算$\log(y)$时，$y$为$0$与非$0$的区别是特别大的。

   **解决方法**：在计算$\log(y_j)$时，避免用$\exp(x_j- \max(x))$的结果，
   $$
   \begin{align}
   \log y_j &= \log \frac{\exp(x_j- \max(x))  }{\sum_k\exp(x_k- \max(x))} \\
   &= x_j- \max(x) -  \log \left( \sum_k\exp(x_k- \max(x)) \right)
   \end{align}
   $$

因此，为了同时解决数值上溢和下溢问题，损失函数和交叉熵需要分别计算，不能直接通过损失函数的结果计算交叉熵。