# 第八章 循环神经网络

## 序列

**序列数据**：具有**时间或顺序依赖性**的数据。核心特性是，当前的输出不仅依赖当前输入，还可能依赖历史的输入。



## 文本预处理

文本预处理步骤：

1. 读取数据集：将文本作为字符串加载到内存中；
2. 分词：将字符串拆分为**词元（token）**；
3. 词表构建：建立一个**词表（vocabulary）**，将拆分的词元映射到数字索引序列；
4. 编码文本序列：将文本转换为数字索引序列，以方便模型操作。



## 语言模型

目标：构建一个**语言模型**，根据序列中之前的词源，预测序列中**下一个词元**的概率分布；



**建立语言模型**：

给定一个长度为 $T$ 的序列，$[x_1,\cdots, x_T]$；我们希望模型学习出，
$$
P(x_1,\cdots,x_T)=P(x_1)\cdot P(x_2|x_1)\cdots P(x_T|x_1\cdots x_{T-1})
$$
这种对自身进行回归，一步一步预测下一个词元的模型称为自回归模型。



**训练**

为了训练模型，我们需要两个东西：

- 输入序列：词元索引构成的序列。
- 目标序列（标签）：输入序列右移一个词元得到的。

训练的目标是让模型能从输入序列预测出目标序列。



**构造样本对**：

假设我们已经将一个很长的 文本 转换成 词元索引列表 ，如`[1, 5, 2, 3, 6, 7, ...]`。

如果我们想构建训练集训练模型，那就得从这个长序列里**不断地切片，切出多个固定长度的输入序列**，比如每段 5 个字符，然后构造对应的目标序列（右移一位）。

问题：要从哪里开始切出输入序列？下一段输入序列接在哪里？



**采样策略**

采样策略决定了我们从长文本中如何生成多个输入-目标样本对。

假设 `corpus = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`。

1. 随机采样：每次从任意位置随机抽出一段长度为 `num_steps` 的序列作为输入。

   特点：打乱时间顺序，样本独立。

   随机采样得到样本之间没什么联系，例如：

   - `X1 = [2, 3, 4, 5] → Y1 = [3, 4, 5, 6]`
   - `X2 = [0, 1, 2, 3] → Y2 = [1, 2, 3, 4]`

2. 顺序采样：从头开始，**按顺序**一段段分割。

   特点：时间连续性好，适合模拟真实序列生成过程。

   顺序采样得到样本是连续的、顺序的，例如：

   - `X1 = [0, 1, 2, 3] → Y1 = [1, 2, 3, 4]`
   - `X2 = [4, 5, 6, 7] → Y2 = [5, 6, 7, 8]`



**困惑度（Perplexity）**

数学公式：
$$
\text{Perplexity} = \exp\left( -\frac{1}{N}\sum_{i=1}^N \log P(x_i) \right)
$$
其中，$x_i$表示第$i$次预测的正确答案，$P(x_i)$表示第$i$次预测中正确答案的概率，$-\log P(x_i)$可以看作是第$i$次预测的交叉熵损失。

困惑度：衡量语言模型在N次预测中模型的不准确度。

模型预测越准确，困惑度越小，越接近1。

模型越“迷惑”（越不确定），困惑度越大，直至无穷。



## 循环神经网络

全连接神经网络和卷积神经网络：假设输入是“独立同分布”的样本，不擅长处理“前后关联”的数据。

**RNN 的基本动机**：设计一种结构，让“过去的信息”可以传到“现在”来参与决策。



**RNN 的结构设计**：每个时刻有一个“隐状态”（hidden state）$H_t$，当前的输出由当前输入 $x_t$ 以及上一个时刻的隐状态 $H_{t-1}$ 共同决定，如下图所示。

![Fig-8-1](./Fig-8/Fig-8-1.png)



**数学形式**：

对于时间步 $t$，输入是 $x_t$，上一个隐状态是 $H_{t-1}$，更新公式为：
$$
H_t = \phi(W_{xh}x_t + W_{hh}h_{t-1} + b_h) \\
o_t = W_{ho}H_t + b_o
$$
其中，$W_{xh}$是输入到隐藏层的权重；$W_h$是上一个隐状态传给当前隐状态的权重；$h_t$是当前时刻的“记忆”；$o_t$是输出，用来做预测。

所有时间步共享相同的权重 $W_{xh}, W_{hh}, W_{ho}$。



**使用RNN构建语言模型**：

![Fig-8-1](./Fig-8/Fig-8-1.png)



## 循环神经网络的实现

前面我们把词元表示为数字索引，直接将数字索引输入给神经网络是很难训练学习的，通常的做法是**将词元或者数字索引表示为更具有表达力的特征向量**，比如通过独热编码的方式。



**预热**：在进行预测的时候，我们首先把输入传入模型，让模型不断地将隐状态传递到下一个时间步，但是不产生输出，这个过程称为预热期。预热期结束后，隐状态的值通常更适合预测。



在 RNN 训练和预测时，隐状态（hidden state）`h_t` 是如何初始化的？是否每次都重置？

训练时：**每个样本序列（小批量）都要重新初始化状态**

因为训练样本间没有自然顺序，我们**无法依赖上一个 batch 的状态**