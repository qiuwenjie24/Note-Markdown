# 第四章 多层感知机

## 基本概念

**多层感知机**：由多层神经元组成，是最简单的全连接神经网络。

**泛化能力**：训练好的模型在应对测试集或没见过的数据集上的表现能力。

**表达能力**：模型的表达能力指的是拟合函数的能力，能拟合越复杂的函数，模型表达能力越强。

**对数均方根误差(logRMSE)**：当关心的是相对误差$(y-\hat{y})/y$，而不是绝对误差$y-\hat{y}$时，常使用其表示当前的损失,
$$
L=\sqrt{\frac{1}{N}\sum_{i=1}^{N} (\log(y_i)-\log(\hat{y_i})^2)}
$$


## 激活函数

激活函数模仿了生物神经元的工作方式，当神经元的输入信号达到某个阈值时，它才会“激活”并传递信号。其主要作用是引入非线性，帮助神经网络拟合复杂的函数关系。

**常见的激活函数**：

1. ReLU函数，
   $$
   \text{ReLU}(x)=\max(x,0)
   $$
   **优点**：计算简单，且在正区域内梯度不会消失。

   **缺点**：对于负输入输出为0，可能导致“神经元死亡”问题。

2. Sigmoid函数，
   $$
   \text{sigmoid}(x)=\frac{1}{1+\exp(-x)}
   $$
   **优点**：输出平滑，易于理解。

   **缺点**：容易出现梯度消失问题，导致训练缓慢。

3. tanh函数(双曲正切)，
   $$
   \tanh(x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}
   $$
   **优点**：比Sigmoid有更强的输出范围，避免了Sigmoid的梯度饱和问题。

   **缺点**：仍然可能会面临梯度消失问题。

4. Leaky ReLU函数，
   $$
   \text{Leaky ReLU}(x)=\max(\alpha x,0)
   $$
   其中$\alpha$是一个很小的常数，如$0.01$。

   **优点**：缓解了ReLU中的“神经元死亡”问题。

   

## 模型选择与$K$折交叉验证

通常，我们在处理同一件事件时，可以使用不同的方法，构建不同的模型。我需要对模型进行评估，选择出最佳的模型，这个过程称为模型选择。

原则上，在模型选择的过程不希望使用测试集数据，因为会有过拟合测试数据的风险。常见的做法是使用$K$折交叉验证。

**$K$折交叉验证**：

1. 将原始训练数据分成$K$个不重叠的子集；
2. 执行$K$次模型训练和验证，其中在执行第$i$次时，把第$i$个子集作为验证集，剩余的$K-1$个子集作为训练集，先进行训练，然后进行验证，并记录每次的训练误差和验证误差；
3. 对$K$次实验的训练误差和验证误差取平均，通过平均训练误差和平均验证误差进行模型选择。



## 过拟合与欠拟合

模型在训练集上的误差比较小，但是在测试集上的误差很大的现象，称之为**过拟合**。

过拟合的本质是模型把训练数据中的噪声也”记住“了。

可能的原因：

1. 模型复杂度。模型太复杂，容易记住样本噪声，导致过拟合。
2. 数据集大小。训练数据太少，则模型越容易记住噪声，导致过拟合。

解决方法：

1. 使用适合复杂度的模型。
2. 增大数据集。训练数据越多，则噪声的影响越小。
3. 使用正则化技术。例如，权重衰减和暂退法。





模型在训练集上的误差和测试集上的误差都很大的现象，且随着训练次数增加无法继续减小误差的现象，称之为**欠拟合**。

欠拟合的原因是模型太简单，表达能力不足，无法拟合出更好的函数使得误差下降。

解决方法是提高模型复杂度，使用更复杂的模型。



## 权重衰减

我们可以通过限制模型复杂度的方式避免过拟合。

那么如何衡量一个函数的复杂度呢？一种合理的方法是，通过函数与零的距离来衡量函数的复杂度。比如$f(\mathbf{x})=\mathbf{w}^\top \mathbf{x} + b$，则我们可以使用权重向量的范数$\|\mathbf{w}\|$作为函数的距离，度量其复杂度。

为了使得最终的损失函数$L(\mathbf{w},b)$和权重范数$\|\mathbf{w}\|$尽可能小，常用的做法是把范数作为惩罚项加入到损失函数中，
$$
L_{\text{new}} = L(\mathbf{w},b) + \lambda \|\mathbf{w}\|
$$
其中，$\lambda$称为正则化常数或惩罚系数。

当使用的范数为$L_1$范数时，称为$L_1$正则化；当使用$L_2$范数时，称为$L2$正则化，或权重衰减。



### L1正则化







### L2正则化













## 暂退法（Dropout）

暂退法是一种正则化技术，通过在训练阶段**随机屏蔽（置零）部分神经元**，迫使网络不依赖单一神经元，从而缓解过拟合。

本质是降低神经元之间的互相依赖，即使少了一些神经元也对最终预测影响不大，提高每个神经元的能力，从而提高模型的泛化能力。

为了最大限度使用模型，测试阶段则使用全部神经元。

**数学推导**

1. 训练阶段：随机屏蔽神经元。

   设某层神经元输出为向量为$\mathbf{x}=[x_1,\cdots,x_n]$，屏蔽向量为$\mathbf{r}=[r_1,\cdots,r_n]$，且$r_i \sim \text{B}(1,p)$，其中$p$是人为设置的神经元保留概率，表示$\mathbf{r}$的每一个分量有$p$的概率为1，$1-p$的概率为0。

   Dropout后的输出为：
   $$
   \mathbf{x}_\text{dropout} = \frac{1}{p}(\mathbf{r} \odot \mathbf{x})
   $$
   其中$\odot$为逐元素乘法，系数$1/p$是为了保持前后的输出期望一致，对剩余神经元进行放缩补偿，$E[\mathbf{x}_\text{dropout}]=E[\mathbf{x}]$。

2. 反向传播的梯度计算。

   进行Dropout后，新的梯度与原来梯度之间的关系为：
   $$
   \frac{\partial L}{\partial x_i}=\frac{\partial L}{\partial x_{\text{dropout},i}}\cdot \frac{r_i}{p}
   $$





## 梯度爆炸和梯度消失

损失函数$J$关于参数$\mathbf{w}$的梯度的表达式，涉及到不同激活函数的导数以及权重的相，例如：
$$
\frac{\partial J}{\partial w_1}=\cdots g'_3 \cdot w_3\cdot g'_2 \cdot w_2
$$
其中，$g'$时激活函数的导数。



**梯度消失**：在反向传播计算梯度时，梯度值过小或者为零，导致参数无法更新。

原因：

- 当使用Sigmoid函数作为激活函数时，导数值最大为0.25，这会导致多个激活函数导数值相乘之后的结果远小于1，梯度近似为0。
- 权重过小，导致梯度近似为0。

解决方法：

- 使用ReLU函数等，导数值更大的函数作为激活函数。

- 使用Xavier初始化，避免初始的参数过小。



**梯度爆炸**：在反向传播计算梯度时，梯度值过大，导致模型不能稳定收敛。

原因：当权重都很大时，多个权重的相乘会导致梯度值很大，超过计算机容许范围。

解决方法：使用Xavier初始化，避免初始的参数过大。



## 参数初始化

深度神经网络中最常用的一种参数初始化方法是Xavier初始化，又称Glorot初始化。

Xavier初始化通过控制权重的方差，使得网络前向传播和反向传播时的各层激活值方差稳定，避免梯度消失或爆炸问题。

==（如果方差逐层增大，则输入的微小变化会对最终预测造成很大的影响，说明梯度过大，容易爆炸。反之，如果方差逐层减小，则改变输入对预测几乎没有影响，说明梯度消失。）==



**数学推导**：

不考虑非线性，以简单的一层线性神经网络为例：$y=Wx+b$。

假设输入$x$均值为$0$，方差为$\gamma^2_1$；权重$W$从分布中抽取，该分布满足均值为$0$，方差为$\sigma^2$；且$x$与$W$互相独立。

正向传播中，输出$y$的均值和方差分别为：
$$
\begin{align}
E[y_i] &= \sum_i^{n_{in}} E[W_{ij}x_j]=\sum_i^{n_{in}} E[W_{ij}] E[x_j]=0 \\
\text{Var}[y_i] &= E[y_i^2]-(E[y_i])^2=\sum_i^{n_{in}}E[W_{ij}^2]E[x_j^2]=n_{in}\sigma^2\gamma^2
\end{align}
$$
为确保激活值$y_i$的方差稳定，令：
$$
n_{in}\sigma^2=1 \Longrightarrow \sigma^2=\frac{1}{n_{in}}
$$
反向传播中，需要计算梯度$\nabla_w L$：
$$
\frac{\partial L}{\partial W_{ij}} = x_j\cdot\frac{\partial L}{\partial y_j}
$$
可以看到，梯度的稳定依赖于$x_j$和$\frac{\partial L}{\partial y_j}$，而上面已经在向前传播过程做了约束，保证了$x_j$的稳定性。这一层网络的输出的$y$可以看作下一层的网络的输出$x$，而输入可以看作上一层的输出，因此，要保证反向传播中每层的$\frac{\partial L}{\partial y_i} $和$\frac{\partial L}{\partial x_j}$是稳定的。

 $\frac{\partial L}{\partial y_i} $与$\frac{\partial L}{\partial x_j}$的关系为，
$$
\frac{\partial L}{\partial x_j} =  \sum_i^{n_{out}} W_{ij}\cdot \frac{\partial L}{\partial y_i}
$$
假设$\frac{\partial L}{\partial y_i}$满足均值为$0$，方差为$\gamma^2_2$，则反向传播中，输入梯度$\frac{\partial L}{\partial x_j}$的均值和方差分别为：
$$
\begin{align}
E[\frac{\partial L}{\partial x_j}] &= \sum_i^{n_{out}} W_{ij}\cdot E[\frac{\partial L}{\partial y_i}]=0\\
\text{Var}[\frac{\partial L}{\partial x_j}] &= n_{out} \cdot \text{Var}[W_{ij}] \cdot \text{Var}[\frac{\partial L}{\partial y_i}]=n_{out}\sigma^2 \gamma^2_2
\end{align}
$$
为保持梯度方差一致，令：
$$
n_{out}\sigma^2=1 \Longrightarrow \sigma^2=\frac{1}{n_{out}}
$$
为了尽可能让正向传播和反向传播的约束都尽可能满足，最终取两个条件的调和平均：
$$
(n_{in}+n_{out})\sigma^2=2 \Longrightarrow \text{Var}[W]=\sigma^2=\frac{2}{n_{in}+n_{out}}
$$


**初始化分布**

Xavier初始化只对分布的均值和方差做了约束，实际初始化的实现中，常使用均匀分布或正态分布。



**适用范围**

适用于激活函数为 **线性或近似线性**（如Tanh、Sigmoid）的情况；对于对ReLU等非线性函数，需要使用其他初始化。



## 项目流程

1. **下载和加载数据集**

2. **数据预处理**

   - 划分数据集。将原始数据划分为训练集和测试集。
   - 异常值检测与处理。
   - 缺失值填充。
   - 特征工程。
   - 数值特征标准化/归一化。
   - 类别变量处理，如独热编码。
   - 转换成pytorch张量形式，方便后续数据加载处理处理。

   ==上述操作仅在训练集上处理，然后应用到测试集，不重新计算。==

3. **训练过程**

   - 定义模型、损失函数、训练数据迭代器
   - 模型参数初始化。使用 `nn.Sequential()` 或其他 `nn.Module` 子类，构建模型时，参数会自动初始化，无需显式操作。
   - 确定训练回合、批量大小、学习率、权重衰减系数(惩罚系数)。
   - 选择训练优化器。
   - 定义训练过程：加载批量，梯度清零，计算损失，反向传播，梯度更新。
   - 记录每个回合的训练数据损失值和测试数据损失值。这里的测试数据是带标签的，也可以称为验证数据。

4. **K折交叉验证**

   - 将数据集划为$K$份，选择第$i$份为验证数据，剩余$(K-1)$份为训练数据。
   - 使用划分好的训练数据和验证数据进行训练，并记录每个回合的训练损失和验证损失。
   - 重复上面两个步骤，遍历每份数据，$i=1,2,\cdots,K$，使得每份数据都作为一次测试数据。
   - 将得到的$K$份损失进行平均，得到每个回合的平均训练损失和验证损失。

5. **模型选择**

   使用一组固定的超参数，快速比较不同模型的效果，步骤如下：

   - 首先使用线性回归(或其他简单模型)进行数据拟合：
     - 如果模型有较好的表现，则继续改进模型，使用更复杂的模型。
     - 如果模型效果不好，考虑检查数据问题，进行数据清洗。
     - 如果数据处理没有问题，但模型效果比随机猜测更差，说明特征与目标之间可能没有任何有效的关联。
   - 尝试使用其他更复杂的模型：
     - 复杂模型比线性模型效果好，则说明数据中存在非线性关系，复杂模型能捕捉到这些关系。
     - 复杂模型没有带来更好的效果，则需要重新审视数据、特征以及模型调优等手段。

6. **超参数调优**

   通常需要优化的超参数有，学习率、批量大小、优化器、正则化、模型特定超参数(如隐藏层大小)。调优的方法有：

   - 网格搜索：1.为每个超参数定义一个范围；2.穷举每个离散点组合，在每组超参数下进行训练，通过交叉验证评估模型性能；3.选择在测试集（验证集）上表现最好的一组超参数。适用于小规模任务。
   - 随机搜索：与网络搜索类似，但不遍历所有的组合，而是随机选取其中一些组合，在这些组合中找到最优的一组。适用于大规模任务。
   - 贝叶斯优化：根据历史实验的结果来推测哪些超参数组合更有可能产生好结果，从而提高搜索效率。
   - 自动化超参数调优工具：Hyperopt, Optuna。

7. **保存模型**

   - 选择一组最佳的超参数和模型，用全部(带标签的)数据集训练模型。
   - 将模型参数保存。

8. **预测**

   - 选择最佳的超参数，用全部(带标签的)数据集训练模型。
   - 将模型应用于(不带标签的)测试集。
   
9. **特征重要性分析**

   随机选择一些训练样本，计算每个特征的平均梯度，通过梯度分析每个特征的重要性以及对模型预测的影响。







**数据预处理问题**：

1. 数据预处理的时候不能将训练集和测试集进行合并一起处理。

   数据预处理时候，应严格遵循 **“仅在训练集上计算预处理参数，再应用到测试集”** 的原则。原因是，若将数据集和测试集一起处理，则测试集的信息会泄露到训练过程中，从而导致过拟合，模型评估结果过于乐观，失去泛化性；当模型在真实场景中面对新数据时，因为无法提前知道测试数据的均值和标准差，性能会显著下降。

2. 测试集不能独自预处理。

   在预处理过程中，测试集必须**严格使用训练集计算得到的参数**进行处理（如均值、标准差、填充值等），**不能独立预处理**。直接对测试集独自预处理会导致数据分布与训练集不一致，模型评估结果不可靠。







**标准化与填充**：

1. 先填充再标准化：填充之后均值不变，方差变小，之后根据非缺失值和缺失值的填充进行标准化。
2. 先标准化再填充：根据非缺失值进行标准化，之后填充，使得方差变小。

相同点：两种方式都是把缺失值认为是非缺失部分的均值。

异同点：第一种方法标准化的时候考虑了缺失值部分，而第二种方法是只考虑非缺失值部分，从而导致方差偏小。如果非缺失值的均值与真实均值有偏差，后者的方差偏小会放大这个偏差。

- 若数据缺失比例低（<5%），可以先用特征均值填充缺失值，再进行标准化。
- 若数据缺失比例低（<5%），可以先进行标准化，再用特征均值(即零)填充缺失值。
- 若缺失值极少（<1%），可直接删除样本；
- 若某特征缺失率极高（>80%），可考虑删除该特征。



**标准化数据的原因**：

1.尺寸大的特征对预测结果影响大，意味着这个特征是比较重要，但是实际上我们并不知道哪个特征更重要，因此我们应该把所有特征都看成一样重要，缩放到同一个尺度上，通过模型训练中不断地调整参数，从而改变不同特征对预测的影响权重。

2.从优化角度看，尺寸小的特征对预测结果影响小，相应的梯度也就小，这会导致该特征的变化对参数更新几乎无影响，相当于没有这个特征，这会影响模型的优化。





**超参数中回合数不宜太大的原因**：
训练模型的目的是希望找到最佳的一组模型参数，使得总体(真实)分布的损失函数达到最小值。但是我们只有部分数据集(训练集)，只能够构建样本分布的损失函数，并且寻找样本的损失函数的最小值。

需要注意的是，样本数据带有噪声，这使得样本分布的损失函数与总体分布的损失函数有偏差。当样本的损失函数达到最小值时，总体的损失函数可能很大，这时候就会过拟合。因此，我们是在慢慢靠近样本损失最小值的过程中寻找使得总体损失也尽可能小的点。



**特征工程问题**：

特征工程中，构建时间周期特征需要同时使用正弦和余弦函数。通过同时使用正弦和余弦函数，可以将年份映射到二维单位圆上的坐标，从而唯一表示时间点的相位。如果只用正弦或余弦，则无法区分周期内的不同相位。