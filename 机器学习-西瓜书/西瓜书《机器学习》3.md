西瓜书《机器学习》

## 线性模型

### 线性模型

线性模型试图找到一个通过属性的线性组合来进行预测的函数。
$$
f(\mathbf{x})=\mathbf{w}^\top x + b,
$$
其中$\mathbf{w},b$是通过训练学习得到的模型参数。

通常采用均方误差函数作为模型的性能度量。

基于均方误差最小化进行模型求解的方法称为“最小二乘法”。在线性回归中，最小二乘法就是找到一条直线，是所有样本到直线上的欧式距离之和最小，所求解得到的$\mathbf{w},b$称为最小二乘“参数估计”。

----



### 广义线性模型

更一般，考虑单调可微函数$g(\cdot)$，令
$$
y=g^{-1}(\mathbf{w}^\top x + b),
$$
这样得到的模型称为“广义线性模型”，其中$g(\cdot)$称为“联系函数”（link function）。

----



### 对数几率回归（逻辑回归）

若考虑二分类问题，我们希望函数 $g$ 能将线性回归的预测值映射到两个值，如$y\in\{0，1\}$。最理想的函数就是“单位阶跃函数”：当$z<0$，$y=0$；当$z=0$，$y=0.5$；当$z>0$，$y=1$。其中由于$z=0$的测度为0，这种情况下$y$取什么都无关紧要。

我们希望函数是单调且可微的，因此使用对数几率函数替代：
$$
y=\frac{1}{1+\mathrm{e}^{-z}}.
$$
用这个函数替代意味着，我们假定了在所使用的数据中，$y$的取值要么是接近1的，要么是接近0，处于中间值的数据几乎没有，即$P(0<y<1)\approx 0$。



使用对数几率函数后，线性模型可以表示为：
$$
\ln \frac{y}{1-y}=\mathbf{w}^\top x +b,
$$
若将$y$视为样本$x$作为正例（取值为1）的概率，则$1-y$是取反例的概率，两者的比值$\frac{y}{1-y}$称为几率，$\ln \frac{y}{1-y}$称为对数几率。因此，该模型称为对数几率回归，而逻辑回归是音译翻译。

-----



### 线性判别分析（LDA）

LDA的思想：给定训练样本，设法将样本投影到一条直线上，使得同类样本的投影点尽可能接近，而异类样本的投影点尽可能远离；对于新样本，通过将其投影到该条直线上的位置判断所属类别。

实际计算中，通过协方差反映同类样本点的靠近程度，通过均值反映异类样本点的远离程度。

