西瓜书《机器学习》

## 神经网络

简单介绍一些概念。

BP网络：指用BP算法训练的多层前馈神经网络。

缓解BP网络过拟合的策略：

1. 早停：将数据分为训练集和验证集，若连续几个回合验证集的误差都没有继续往下降，则停止训练。
2. 正则化：基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分，例如权重和偏置的平方和。



训练网络很容易落入局部最优点，跳出局部最优点的策略有：

1. 多组不同的参数初始化网络。不同初始点可能会落入不同的局部最优点，然后从中选择最好的局部最最优点。
2. 模拟退火算法。在每一步都以一定的概率接受比当前解更差的结果，从而有助于跳出局部最最优点。（更详细地这里不展开说）
3. 随机梯度下降。随机抽取样本进行梯度下降算法，由于抽样的随机性导致梯度不稳定，容易跑出最优点。
4. 遗传算法。（不懂）



增量学习：指在学的模型后，再接收到训练样本时，仅需要根据新样本对模型进行更新，不必重新训练整个模型，并且先前学得到的有效信息不会被“冲掉”。



在线学习：指每获得一个新样本就进行一个模型更新。



增量学习可视为“批模式”的在线学习。



无监督逐层训练是多隐层网络训练的有效是手段，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，这称为“预训练”；在预训练全部完成之后，再对整个网络进行“微调”训练。



“预训练+微调”的做法可视为将大量参数分组，对每组先找到局部看起来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优。