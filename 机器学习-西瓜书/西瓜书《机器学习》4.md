西瓜书《机器学习》

## 决策树

### 基本流程

决策树的基本原理就是每次选择某个特征（属性），对数据进行分割，然后不断重复这个过程，直到分割的每一部分都满足停止条件。  

**目标**：找到一系列规则，将数据逐步分类或预测数值。

**树的结构**：由根节点、内部节点和叶节点组成。根节点包含所有数据，是树的起止点；经过特征选择之后数据被划分成几个部分，每个部分称为节点；其中达到停止条件，能够得到决策结果的节点称为叶节点；剩下的称为内部节点，仍可继续分割。

**特征选择**：每次选择的特征以及划分的方式是根据数据的“纯度”决定的。“纯度”反映的是选择这种划分后，节点的样本（分割后的每一部分数据）都属于同一类别的程度。用于反映“纯度”的指标有信息增益（ID3算法）、信息增益率（C4.5算法）、基尼系数（CART算法）和均方误差（用于回归树，节点输出值是连续的情形）。

**停止条件**：1.节点的样本全部属于同一类，此时该类别就是节点的决策结果；2.达到设置好的最大深度或最小样本数，此时常选择包含样本数最多的类别作为节点决策结果。



**递归分割过程**

1. **从根节点开始**，遍历所有特征和可能的分割点（如“年龄”的每个取值）。
2. **选择最优分割**：计算每种分割后的纯度，选择最优特征和阈值。
3. **生成子节点**：根据分割规则将数据划分为左右子树（也可以划分为更多的子树）。
4. **重复步骤1-3**，直到满足停止条件（如节点样本数<5，或深度达到3）。



**优点**：模拟人类决策过程，具有直观、易解释的特点。

注意：在一条分支中，同一个特征是也可以多次使用的，只要决策规则不是完全相同就可以了，比如`身高<1.7`的子分支中可能会出现`身高<1.6`的节点。

----



### 剪枝处理

剪枝（pruning）是决策树学习算法对付“过拟合”的主要手段。决策树的分支太多容易导致模型过于“记住”训练样本，出现过拟合现象。

决策树剪枝的策略有：**预剪枝**和**后剪枝**。

**预剪枝**：在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点。

注意，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高，因此，预剪枝会导致模型有欠拟合的风险。

**后剪枝**：先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点（内部节点）进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点。

后剪枝是自下而上的方式进行处理，先处理子节点，如果子节点被保留，父节点的剪枝决策会考虑到子节点的结构。

后剪枝决策树通常比预剪枝决策树保留了更多的分支，泛化效果更好，但是训练时间的开销也更大。

----



### 连续值与缺失值

**特征连续值处理**：

决策树中的连续值特征通常采用二分法分割，将连续值归属到不同的区间上。

具体操作：将连续值排序后，取相邻值的**中点**作为候选分割阈值，计算每个阈值的信息增益/基尼系数，选择最优分割点。



**特征缺失值处理**：

简单地放弃不完整样本，仅使用无缺失值的样本进行学习，显然是对数据信息极大的浪费，因此缺失值的处理很重要。



 决策树专属方法——**C4.5算法**:

- 将缺失值视为一个特殊类别，参与信息增益计算。
- 按非缺失样本的比例分配信息增益.;

----



### 多变量决策树

若我们把每个特征视为坐标空间中的一个坐标轴，则$d$个特征描述的样本就对应了$d$维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界。

决策树的分类边界的特点：轴平行，即它的分类边界由若干个与坐标轴平行的分段组成。

若能使用斜边划分边界，则能够使得决策树模型大大简化，由此，提出了“多变量决策树”。

多变量决策树的内部节点不再是仅对某个特征属性，而是对特征属性的组合进行测试，是一个线性分类器。

在多变量决策树的学习过程中，不是为每个内部节点寻找一个最优划分特征，而是试图建立一个合适的线性分类器。