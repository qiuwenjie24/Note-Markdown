西瓜书《机器学习》

## 支持向量机

### 支持向量机的数学形式

给定训练集$D=\{(x_1,y_1),\cdots\},y_i\in \{-1,+1\}$。 

**支持向量机（SVM）**的基本思想：在样本空间中找到一个最优的超平面，将不同类别的样本分开，其中最优是指不同类的数据到该超平面的距离最大。



**支持向量**：距离超平面最近的的样本点。

**间隔/边际（margin）**：各类别的支持向量到超平面的距离之和，$\gamma=\frac{n}{\|\mathbf{w}\|}$，其中$n$表示类别个数。



**超平面**：$\mathbf{w}\cdot \mathbf{x}+b=0$。

如果超平面能够将训练集分类，则存在常数$c>0$，使得所有样本满足
$$
\begin{align}
&\mathbf{w}\cdot \mathbf{x}_i +b \ge c ,\ y_i= +1\\
&\mathbf{w}\cdot \mathbf{x}_i +b \le -c ,\ y_i= -1\\
\Longleftrightarrow & y_i(\mathbf{w}\cdot \mathbf{x} +b) \ge c
\end{align}
$$
此时，**间隔**为$\gamma=\frac{2c}{\|\mathbf{w}\|}$，。$\mathbf{w},b$同时放缩时，超平面不改变，因此我们​把$c$放入到了$\mathbf{w},b$中  从而得到$\gamma=\frac{2}{\|\mathbf{w}\|}$。

我们的目标是希望找到具有“最大间隔”的超平面，即$\gamma$最大，这等价于最小化$\|\mathbf{w}\|^2$，因此该问题转化为了解决不等式约束的优化问题：
$$
\min_{w,b} \frac{1}{2}\|w\|^2 \\
\text{s.t. }y_i(\mathbf{w}\cdot \mathbf{x} +b) \ge 1
$$
这就是支持向量机的数学形式。

另外，从上面的推导过程可知，只有支持向量对超平面有影响，其他样本无影响。



--------

### 对偶问题

一种高效解决上述问题的方法是通过拉格朗日乘子法将原始优化问题转化为对偶问题。

将约束融入目标函数，引入拉格朗日乘子$α_i≥0$，构造拉格朗日函数：
$$
L(w,b,\alpha)=\frac{1}{2}\|w\|^2 + \sum_{i=1}^m \alpha_i[1-y_i(\mathbf{w}\cdot \mathbf{x}_i +b)]
$$

相应的对偶函数为：
$$
\begin{align}
\Gamma(\alpha)&=\inf_{w,b}\ L(w,b,\alpha) \\
&= \sum_i^m\alpha_i - \frac{1}{2}\sum_i^m\sum_j^m\alpha_i\alpha_jy_iy_j \mathbf{x}_i\cdot \mathbf{x}_j
\end{align}
$$
对偶问题为：
$$
\max_\alpha\ \Gamma(\alpha)\\
\text{s.t. } \sum_i^m \alpha_iy_i=0,\ \ \alpha_i\ge 0
$$
超平面为：
$$
f(\mathbf{x})=\mathbf{w}\cdot \mathbf{x}+b=\sum_i^m\alpha_i y_i \mathbf{x}_i\cdot \mathbf{x}
$$
因为满足强对偶条件，对偶问题的最优解就是原问题的最优解。常采用序列最小最优化(SMO)算法解决对偶问题。

计算对偶函数的过程以及KKT条件给出了拉格朗日乘子$\alpha_i$与$w,b$之间的关系。



--------

### 核函数

在原始样本空间中可能不存在一个能够正确划分样本类别的超平面，这时，我们可以将样本映射到一个更高维的特征空间，使得这个高维空间是线性可分。

设$\phi(x)$表示将样本$x$映射后的特征向量，则高维空间的划分超平面为：
$$
f(x)=\mathbf{w}\cdot \phi(\mathbf{x}) + b
$$
优化问题为：
$$
\min_{w,b} \frac{1}{2}\|w\|^2 \\
\text{s.t. }y_i(\mathbf{w}\cdot \phi(\mathbf{x}) +b) \ge 1
$$
对偶问题为：
$$
\max_\alpha\ \sum_i^m\alpha_i - \frac{1}{2}\sum_i^m\sum_j^m\alpha_i\alpha_jy_iy_j \phi(\mathbf{x}_i)\cdot \phi(\mathbf{x}_j)\\
\text{s.t. } \sum_i^m \alpha_iy_i=0,\ \ \alpha_i\ge 0
$$
**困难**：在高维空间计算内积$\phi(\mathbf{x}_i)\cdot \phi(\mathbf{x}_j)$是困难的。

为了解决该困难，定义核函数：
$$
\kappa(\mathbf{x}_i,\mathbf{x}_j)=\phi(\mathbf{x}_i)\cdot \phi(\mathbf{x}_j),
$$
只要知道了核函数，就可以在原样本空间计算，然后再通过核函数映射得到高维空间上的内积。

高维空间的超平面为：
$$
f(\mathbf{x})=\mathbf{w}\cdot \phi(\mathbf{x}) +b=\sum_i^m\alpha_i y_i \kappa(\mathbf{x}_i,\mathbf{x})
$$
只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。核函数的选择是支持向量机的最大变数。

**常见核函数**：线性核（无映射），多项式核，高斯核（RBF）等。



-------

### 软间隔

**困难**：样本中总是会带有噪声，以及存在异常点，这使得不同类别的样本点会比较靠近，导致间隔很小，容易过拟合。如果该异常点甚至在另一类别中范围中，类别间存在重叠，则无法线性区分数据。

**解决**：对于异常点，我们无需让它们都在间隔之外，即允许一些样本违反间隔边界，但对违反程度进行惩罚，平衡分类准确性和模型复杂度。



此时，我希望找到超平面满足，
$$
\begin{align}
&\mathbf{w}\cdot \mathbf{x}_i +b \ge +(1-\xi_i) ,\ y_i= +1\\
&\mathbf{w}\cdot \mathbf{x}_i +b \le -(1-\xi_i),\ y_i= -1\\
\Longleftrightarrow & y_i(\mathbf{w}\cdot \mathbf{x} +b) \ge 1-\xi_i
\end{align}
$$
其中我们引入**松弛变量**$\xi_i(>=0)$，表示第$i$个样本违反间隔约束的程度，可以理解成违反点到间隔边界的“距离”，但这个“距离”的表示方式有很多种。

- $\xi_i=0$表示该样本在间隔之外，满足硬间隔约束。
- $0<\xi_i<0$表示样本在间隔之内，不满足硬间隔约束，但是仍在超平面正确的一侧，可以被正确分类。
- $\xi_i\ge1$表示样本在超平面上，或者跑到了另一端，样本会认认为是另一端的类别，被错误分类。（==这些点基本都是异常点，数量很少，被错误分类也影响不大==）
- $\xi_i$常选择为$\xi_i=\max\{0, 1-y_i(w_i\cdot x_i+b)\}$，如果是满足约束的点，则$y_i(w_i\cdot x_i+b)\ge1$，贡献为0。

我们的优化目标是希望间隔尽可能大，同时不满足约束的样本尽可能少：
$$
\min_{w,b,\xi_i} \frac{1}{2}\|w\|^2 + C\sum_{i}\xi_i
$$
其中$C$是惩罚参数，表示违反约束的容忍度。

因此，软间隔的优化问题为：
$$
\min_{w,b,\xi_i} \frac{1}{2}\|w\|^2 + C\sum_{i}\xi_i \\
\text{s.t. }y_i(\mathbf{w}\cdot \mathbf{x} +b) \ge 1-\xi_i,\ \text{和}\ \xi\ge0
$$
同样的方式，可以推导出相应的对偶问题。





-----

### 总结

**支持向量机通过最大化间隔（边际）的优化策略寻找分类边界，而支持向量是决定这一边界的关键样本点**。核函数和软间隔的引入使其能灵活处理复杂问题。